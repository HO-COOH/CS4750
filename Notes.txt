heuristic 启发式的
1.
    Intelligent Agent
        definition: An Intelligent agent perceives its environment via sensors and acts rationally upon that environment with its actuators
        examples: Human
        Agent Function: Maps percept (P) sequences to actions (A)
            f: P* -> A
        Agent program: Runs on the physical architecture to produce f
        Rationality: 
            What is rational depends on: Performance measure, Prior environment knowledge, Actions, Percept sequence to date (sensors)
            A rational agent chooses whichever action maximizes the expected value of the performance measures given the percept sequence to date and prior environment knowledge
        Environments: To design a rational agent we must specify its task environment
            Performance
            Environment:
                Fully observable <-> partially observable : whether the sensors can detect all aspects that are relevant to the choice of action
                Deterministic <-> stochastic: if the next state is completely determined by the current state and the executed action, then it is deterministic
                Episodic <-> sequential: in an episodic environment the agent's experience can be divided into atomic steps where the agents perceives and then performs a single action
                Static <-> dynamic: if the environment can change while the agent is choosing an action, the environment is dynamic
                Discrete <-> continuous: the way time is handled and to the percepts/actions of the agent
                Single <-> Multi-agent: whether the environment contain other agents who are also maximizing some performance measure that depends on the current agent's actions
            Actuators
            Sensors
        Agent types:
            Table-Driven agent:
                lookup(percepts, table)->action
                return action
            Simple reflex agents: select action only by the current percept
            Model-based agents: maintains internal state and update it to tackle partially observable environment
            Goal-based agents: future is taken into account to maximize a goal
            Learning agents:

2.searching method
    problem-splving agents:
        problem types:
            deterministic, fully observable                 -> single-state problem
            non-observable                                  -> conformation problem
            nondetermministic and/or partially observable   -> contingency problem
                percepts provide new information about current state
            unknown state space                             -> exploration problem
        problem formulation:
            initial state(s0)
            action function(s) -> {a1, a2, a3...}   //take a state (s) as an input and return a set of actions ({a1,a2,a3...}) possible to take
            result function(s,a) -> s'              //take a state and an action (s,a) as inputs and return a new state (s')
            goalTest function(s) -> bool            //take a state (s) as an input and check whether it has reached the goal state, return a boolean value
            pathCost function(s1+a -> s2) ->cost n      //take a state and an action (s1+a) and determine the cost to reach to the next state(s2), return the cost new 
        exmaples:
    basic search algorithms:
        Tree search algorithm:
            function Tree-Search(problem, strategy) retuns a solution or failure
                initialize the search tree using the initial state of problem
                loop:
                    if there are no candidates for expansion 
                        return failure
                    choose a leaf node for expansion according to strategy
                    if the node contains a goal state 
                        return the corresponding solution
                    else
                        expand the node and add the resulting nodes to the search tree
        comparison standards:
            completeness
            time complexity
            space complexity
            optimality
        Interactive deepening search: Save space compared to breadth first search because it forgets prior result
        Tree search -> Graph search
    Local Search Algorithms
        Hill-climbing
            pseudocode:
                Hill-climbing(problem) -> state
                {   Node current = initial_state, neighbor
                    while(1)
                    {
                        neighbor = max(current.successor)
                        if(neighbor.value <= current.value)
                            return current
                        else
                            current=neighbor
                    }
                }
            hill-climbing will get stuck at a local-maximum

        Simulated annealing
            pseudocode:
                Simulated-annealing(problem, schedule) -> state     //schedule is a function mapping from time to temperature: f(time) -> temperature
                {
                    Node current = initial_state, next
                    Temperature T
                    for Time t = 1:∞
                    {
                        T = schedule(t)
                        if(T == 0)
                            return current
                        else
                        {
                            next = randomly_select(current.successor)
                            E = next.value - current.value
                            if(E > 0)
                                current=next
                            else
                                current=next with a probability of exp(E/T) 
                        }
                    }
                }

    Game Search:
        Minimax
            Limitations: perfect play for deterministic, perfect-information games
            Idea: choose move to position with highest minimax value -> best achievable payoff against best play
            pseudocode:
                Minimax-decision(state) -> action
                {
                    return action(state) which maximizing Min-value(action(state).result)
                }
                Max-value(state) -> utility value
                {
                    if(state.is_terminal())
                        return Utility(state)
                    value v = -inf
                    for_each successor in state.successors
                    {
                        v=Max-value(Min-value(successor))
                    }
                    return v
                }
                Min-value(state) -> utility value
                {
                    if(state.is_terminal())
                        return Utility(state)
                    value v = +inf
                    for_each successor in state.successors
                    {
                        v=Min-value(Max-value(successor))
                    }
                    return v
                }
            alpha-beta pruning
                Idea: because minimax search is basically depth-first tree search, and it is time consuming, some branches do not need to be explored
                pseudocode:
                Alpha-beta-decision(state) -> action
                {
                    return action(state) which maximizing Min-value(action(state).result)
                }
                Max-value(state, alpha, beta) -> utility value
                {
                    if(state.is_terminal())
                        return Utility(state)
                    value v = -inf
                    for_each successor in state.successors
                    {
                        v=Max-value(Min-value(s, alpha, beta))
                        if(v >= beta)                           //if one of the successor of state has a value v which is bigger than previous minimum value, than this successor doesn't need to be  
                            return v
                        a = max(alpha, v)
                    }
                    return v
                }



    



!!
1. Greedy best search is not a special case of A* search.
    Reson: Because greedy search only use heuristic function h(n), and A* search uses f(n)=g(n)+h(n), where g(n) is the real cost to the current state.
    g(n) can't be set to 0 so greedy search can't become A* search.
2. Depth first search is not a special case of A* search.
    Reason: The same as above.
3. Uniform cost search is a special case of A* search.
    Reason: h(n) can be set to 0, and it is always admissable.
