heuristic 启发式的
1.
    Intelligent Agent
        definition: An Intelligent agent perceives its environment via sensors and acts rationally upon that environment with its actuators
        examples: Human
        Agent Function: Maps percept (P) sequences to actions (A)
            f: P* -> A
        Agent program: Runs on the physical architecture to produce f
        Rationality: 
            What is rational depends on: Performance measure, Prior environment knowledge, Actions, Percept sequence to date (sensors)
            A rational agent chooses whichever action maximizes the expected value of the performance measures given the percept sequence to date and prior environment knowledge
        Environments: To design a rational agent we must specify its task environment
            Performance
            Environment:
                Fully observable <-> partially observable : whether the sensors can detect all aspects that are relevant to the choice of action
                Deterministic <-> stochastic: if the next state is completely determined by the current state and the executed action, then it is deterministic
                Episodic <-> sequential: in an episodic environment the agent's experience can be divided into atomic steps where the agents perceives and then performs a single action
                Static <-> dynamic: if the environment can change while the agent is choosing an action, the environment is dynamic
                Discrete <-> continuous: the way time is handled and to the percepts/actions of the agent
                Single <-> Multi-agent: whether the environment contain other agents who are also maximizing some performance measure that depends on the current agent's actions
            Actuators
            Sensors
        Agent types:
            Table-Driven agent:
                lookup(percepts, table)->action
                return action
            Simple reflex agents: select action only by the current percept
            Model-based agents: maintains internal state and update it to tackle partially observable environment
            Goal-based agents: future is taken into account to maximize a goal
            Learning agents:

2.searching method
    problem-splving agents:
        problem types:
            deterministic, fully observable                 -> single-state problem
            non-observable                                  -> conformation problem
            nondetermministic and/or partially observable   -> contingency problem
                percepts provide new information about current state
            unknown state space                             -> exploration problem
        problem formulation:
            initial state(s0)
            action function(s) -> {a1, a2, a3...}   //take a state (s) as an input and return a set of actions ({a1,a2,a3...}) possible to take
            result function(s,a) -> s'              //take a state and an action (s,a) as inputs and return a new state (s')
            goalTest function(s) -> bool            //take a state (s) as an input and check whether it has reached the goal state, return a boolean value
            pathCost function(s1+a -> s2) ->cost n      //take a state and an action (s1+a) and determine the cost to reach to the next state(s2), return the cost new 
        exmaples:
    basic search algorithms:
        Tree search algorithm:
            function Tree-Search(problem, strategy) retuns a solution or failure
                initialize the search tree using the initial state of problem
                loop:
                    if there are no candidates for expansion 
                        return failure
                    choose a leaf node for expansion according to strategy
                    if the node contains a goal state 
                        return the corresponding solution
                    else
                        expand the node and add the resulting nodes to the search tree
        comparison standards:
            completeness
            time complexity
            space complexity
            optimality
        Interactive deepening search: Save space compared to breadth first search because it forgets prior result
        Tree search -> Graph search
    Local Search Algorithms
        Hill-climbing
            pseudocode:
                Hill-climbing(problem) -> state
                {   Node current = initial_state, neighbor
                    while(1)
                    {
                        neighbor = max(current.successor)
                        if(neighbor.value <= current.value)
                            return current
                        else
                            current=neighbor
                    }
                }
            hill-climbing will get stuck at a local-maximum

        Simulated annealing
            pseudocode:
                Simulated-annealing(problem, schedule) -> state     //schedule is a function mapping from time to temperature: f(time) -> temperature
                {
                    Node current = initial_state, next
                    Temperature T   //T is decreasing with time t
                    for Time t = 1:∞
                    {
                        T = schedule(t)
                        if(T == 0)
                            return current
                        else
                        {
                            next = randomly_select(current.successor)
                            E = next.value - current.value
                            if(E > 0)
                                current=next
                            else
                                current=next with a probability of exp(E/T) 
                        }
                    }
                }

    Game Search:
        Minimax
            Limitations: perfect play for deterministic, perfect-information games
            Idea: choose move to position with highest minimax value -> best achievable payoff against best play
            pseudocode:
                Minimax-decision(state) -> action
                {
                    return action(state) which maximizing Min-value(action(state).result)
                }
                Max-value(state) -> utility value
                {
                    if(state.is_terminal())
                        return Utility(state)
                    value v = -inf
                    for_each successor in state.successors
                    {
                        v=Max-value(Min-value(successor))
                    }
                    return v
                }
                Min-value(state) -> utility value
                {
                    if(state.is_terminal())
                        return Utility(state)
                    value v = +inf
                    for_each successor in state.successors
                    {
                        v=Min-value(Max-value(successor))
                    }
                    return v
                }
            alpha-beta pruning
                Idea: because minimax search is basically depth-first tree search, and it is time consuming, some branches do not need to be explored
                pseudocode:
                Alpha-beta-decision(state) -> action
                {
                    return action(state) which maximizing Min-value(action(state).result)
                }
                Max-value(state, alpha, beta) -> utility value
                {
                    if(state.is_terminal())
                        return Utility(state)
                    value v = -inf
                    for_each successor in state.successors
                    {
                        v=Max-value(Min-value(s, alpha, beta))
                        if(v >= beta)                           //if one of the successor of state has a value v which is bigger than previous minimum value, than this successor doesn't need to be  
                            return v
                        a = max(alpha, v)
                    }
                    return v
                }

Chapter6 Constrain statisfaction problems
    1. Definition
        state:      defined by variable Xi with values in domain Di
        goal test:  a set of contraints specifying allowable combinations of values for {variables Xi}
    2. Example:
        Map-coloring
    3. Constraint graph
        If each constraints relates at most 2 variables, they can be represented by a binary CSP graph
        Node -> variable
        Arc  -> constraints    
    4. Varieties of CSP
        Discrete variable:
            domain D is finite, size=d
            nodes N is finite, size=n
            So, there are O(d^n) complete assignments
        Continuous variable:
            domain D and nodes N is infinite
        Unary constraints: involve a single variable
        Binary constriants: involve pairs of variables (Map-coloring problem)
        Higher-order constraints: involves >=3 variables
        Soft constraints: has perference\cost\optimizations
    5. Standard search formulation
        /*State: defined by values assigned so far*/
        Initial state: { }
        Successor function: assign value -> unassigned variable that doesn't conflict, return fail if no legal assignment
        Goal test: all variables are assigned
        //Every solution appears at depth n with n variables
        //Path is irrelevant, so we can use depth-first search
        //b=(n-l)d b-branching factor l-depth -> O(n!d^n) leaves!
    6.Back-tracking search
        Only consider assignments to a single variable at each node
        //b=d -> O(d^n) leaves!
        pseudocode:
            Back_tracking_search(CSP) -> solution/failure
            {
                return Recursive_backtracking({ }, CSP)
            }

            Recursive_backtracking(assignment, CSP) -> solution/failure
            {
                if(assignments is complete)
                    return assignments
                variable var=Select an unassigned variable
                for value: Domain d
                {
                    if(value is consistent with CSP.constraints)
                    {
                        var=value
                        assignment+=var
                        result=Recursive_backtracking(assignment, CSP)
                        if(result !=failure)
                            return result
                        else
                            assignment-=var
                    }
                }
                return failure
            }
        Improving backtracking efficency:
            Minimum remaining values(MRV): select the variable with the fewest legal values
                When there are variables that has the same fewest legal values, use degree heuristic
            Degree heuristic: select the variable with the most constraints
            Least constraining value: given a variable, assign it to the value which rules out fewest values in the remaining variables
    7.Forward checking
        Keep yrack of remaining legal values for unassigned variables. Terminate search when any variable has no legal values
        Forward checking propagates information from assigned to unassigned variables, but doesn't provide early detection for all failures
    8. Arc consistency
        Simplest form of propagation makes arc consistent
            X(variable)->Y is consistent if & only if for every value x of X, there is some allowed y for Y
            If X loses a value, neighbors of X need to be rechecked
        Arc consistency detects failure earlier than forward checking. It runs as a preprocessor or after each assignment
        pseudocode:
            AC-3(CSP) -> new CSP with reduced domains
            {
                
            }
Chapter 7 Logical agents:
    0. Entailment
        KB |= a
        is true iff. a is true in all worlds where KB is true
        iff. KB ∈ a
    1. Horn form
        KB=conjunction of horn clauses
        Horn clause=propositional symbol(no negation)/conjunction of symbols + => symbol 
            eg: C^(B => A)^(C^D => B)
    2. Conjuctive Normal Form (CNF)
        conjunction of disjunctions of literals
            eg: (A V !B)^(B V !C V !D)
    3. Resolution Algorithm
        1) Convert KB to CNF
        2) Show KB ^ !a is unsatisfiable

    !!! KB |= a iff. KB^!a is unsatisfiable
        KB |= a iff. KB => a is valid

             

Chapter 8 First Order Logic
    1. FOL vs propositional logic
        propositional logic assumes world contains facts <-> FOL assumes the world contains: Objects\ relations\ functions
    2. Synatx
        Constants: words start with captial letter
        Predicates: accepts two variables and return a T/F value
        Functions:
        Variables: lower case letter
        Connectives: ^ v ! => <=>
        Equality: =
        Quantifiers: For all/ There exist
    3. Atomic sentences
        predicates(V1, V2)
        constant(a single constant)
        variable(a single variable)
    4. Complex sentences
        made from atomic sentences using connectives
    5. Universal quantification
        For any<variable> sentence
            eg: Everyone at Berkeley is smart:
            For any x, At(x, Berkeley) => Smart(x)
            !!! => is the main connective with For any<>, not ^
            For any x, At(x, Berkeley) ^ Smart(x) means: Everyone is at Berkeley and everyone is smart
    6. Existential quantification
        There exist<variable> sentence
            eg: Someone at Stanford is smart.
            There exist x, At(x, Stanford) ^ Smart(x)
            !!! ^ is the main connective with There exist, not =>
            There exist x, At(x, Stanford) => Smart(x) is true if there is anyone who is not at Stanford

    !!!Only (exactly) one person like John
        (There exist x, People(x)^Likes(x, John)) ^ For any y, ( ( (People(y)^Likes(y, John))=>(x=y))) 

Chapter 13 Uncertainty
    1. Basic
        A set: Ω -the sample space
        sample point: w ∈ Ω is an atomic event
        A probability space is a sample space with an assignment P(w) for every w ∈ Ω such that:
            0<=P(w)<=1
            Sum(P(w),w)=1
        An event A is any subset of Ω, and P(A)=Sum(P(w), w∈A)
            



!!
1. Greedy best search is not a special case of A* search.
    Reson: Because greedy search only use heuristic function h(n), and A* search uses f(n)=g(n)+h(n), where g(n) is the real cost to the current state.
    g(n) can't be set to 0 so greedy search can't become A* search.
2. Depth first search is not a special case of A* search.
    Reason: The same as above.
3. Uniform cost search is a special case of A* search.
    Reason: h(n) can be set to 0, and it is always admissable.
